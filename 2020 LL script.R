# File: 2020 LL Script.R
# Purpose: Analysis of Florida Large Lake data generated by the Status 
#  Monitoring Program. Created by Jay Silvanima using code developed 
#  by Tony Olsen, myself, Chris Sedlacek, Stephanie Sunderman-Barnes, 
#  and Liz Miller on 09-26-2020.
# Code developed using R version 3.6.2 (2019-12-12) and spsurvey version
#   4.1.0.

##Set directory. This is where the outputs will be saved. 
#   Alter to desired location. Use getwd() to determine the directory for
#     your r project.

setwd("C:/R/Status 2020/2020 LL")

# First pull data via use of package 'FDEPgetdata'. 

# Load libraries for the data analyses

library(FDEPgetdata)
library(spsurvey)
library(sf)
library(ggplot2)

# Run function of FDEPgetdata package which pull exclusion data.
# 
# Insert varible name between parentheses in function call below.
#
# Insert the name of the exclusion table created in FDEP's Oracle database
#  'GWIS' by Liz Miller. Be sure to enclose table name in single quotes. 

# Example 'LL_EXCLUSIONS_2020'. Note are using 2019 below for a test.

FDEPgetdata::getdata_lake_exclusions('LL_EXCLUSIONS_2019')

# Function getdata_lake_exclusions creates a dataframe names 'Exclusions' from the 
#  information provided. Total nitrogen (F.A.C. 62-302.531), total phosphorus (F.A.C. 62-302.531), 
#  and dissolved oxygen (F.A.C. 62-302.533) criteria are added for each record
#  based on the corresponding nutrient watershed region and bioregion.

# Create new data frame from the one just created.

LL.SITES<-Exclusions
names(LL.SITES)

# Convert to Decimal degrees and do map projection

deg <- floor(LL.SITES$RANDOM_LATITUDE/10000)
min <- floor((LL.SITES$RANDOM_LATITUDE - deg*10000)/100)
sec <- LL.SITES$RANDOM_LATITUDE - deg*10000 - min*100
LL.SITES$latdd <- deg + min/60 + sec/3600
deg <- floor(LL.SITES$RANDOM_LONGITUDE/10000)
min <- floor((LL.SITES$RANDOM_LONGITUDE - deg*10000)/100)
sec <- LL.SITES$RANDOM_LONGITUDE - deg*10000 - min*100
LL.SITES$londd <- deg + min/60 + sec/3600

# Change londd to negative for correct use in sf.
LL.SITES$londd <- -LL.SITES$londd

# Create sf object and transform to Albers projection for analysis
#  This codes utilizes Coordinate Reference System (CRS) Codes.
#  The first crs code below is for NAD 83 Harn datum the second crs code
#  is for Florida albers projection. More information on these codes is found here:
#  https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf.

dsgn_LL <- st_as_sf(LL.SITES, coords = c("londd", "latdd"), remove = FALSE,
                    crs = 4152)
dsgn_sf <- st_transform(dsgn_LL, crs = 3087)

# keep xy coords as variables
tmp <- st_coordinates(dsgn_sf)
dsgn_sf$xcoord <- tmp[, "X"]
dsgn_sf$ycoord <- tmp[, "Y"]

# plot sites using sf
plot(st_geometry(dsgn_sf))

# Site Evaluation
#  The variables CAN_BE_SAMPLED, EXCLUSION_CATEGORY and EXCLUSION_CRITERIA 
#   provide information on the site evaluation results for each site. 
#  Review the information and create target/nontarget (TNT) variable.

addmargins(table(dsgn_sf$EXCLUSION_CATEGORY, dsgn_sf$CAN_BE_SAMPLED, useNA = 'ifany'))
addmargins(table(dsgn_sf$EXCLUSION_CRITERIA, useNA = 'ifany'))


# create sampled and target (T) / nontarget (NT) variables

dsgn_sf$EXCLUSION_CATEGORY <- as.character(dsgn_sf$EXCLUSION_CATEGORY)
dsgn_sf$EXCLUSION_CATEGORY[dsgn_sf$CAN_BE_SAMPLED == 'Y'] <- 'SAMPLED'
dsgn_sf$EXCLUSION_CATEGORY <- as.factor(dsgn_sf$EXCLUSION_CATEGORY)
levels(dsgn_sf$EXCLUSION_CATEGORY)
dsgn_sf$TNT <- dsgn_sf$EXCLUSION_CATEGORY
levels(dsgn_sf$TNT) <- list(T=c('SAMPLED', 'NO PERMISSION FROM OWNER', 'UNABLE TO ACCESS','OTHERWISE UNSAMPLEABLE','DRY'),
                            NT=c('WRONG RESOURCE/NOT PART OF TARGET POPULATION') )

addmargins(table(dsgn_sf$EXCLUSION_CATEGORY, dsgn_sf$TNT, useNA = 'ifany'))

# Adjust weights for Design As Implemented

# Note need frame size here found in design doc for latge lake site selections
#  Z:\Chris site selection process\2020 site selections\2020 LL
#  \Florida Large Lake Design 2020 Documentation.doc.
# From design document framesize in hectates = 384,373.86 for entire data frame.

# 2020 framesize
#framesize <- c("ZONE 1"=18319.077 ,"ZONE 2"=7646.538 ,"ZONE 3"=121141.645,
#               "ZONE 4"=43936.962 ,"ZONE 5"=63328.456 ,"ZONE 6"=130001.179)

# 2019 framesize
framesize <- c("ZONE 1"=19966.323,"ZONE 2"=7654.355,
               "ZONE 3"=121093.811,"ZONE 4"=44508.296,"ZONE 5"=63355.565,
               "ZONE 6"=130028.876)

# use all evaluated sites to adjust weights
nr <- nrow(dsgn_sf)
dsgn_sf$wgt <- adjwgt(rep(TRUE,nr), dsgn_sf$NEST1_WT, 
                      dsgn_sf$REPORTING_UNIT, framesize=framesize)

# check sum of weights for each reporting unit/basin
addmargins(tapply(dsgn_sf$wgt, dsgn_sf$REPORTING_UNIT, sum))


# This gives the weights for the LL design as implememented in 2020. 
# It must include all evaluated sites as some sites are not in the 
#  target population.

# Estimate Extent LL Area.
# Since the sample frame includes portions of LL object polygons that do not 
#  meet the definition of a LL, the site evaluation information is used 
#  to estimate the LL hectares in the target population for entire state and for 
#  each of the reporting units/basins.

sites <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, Use=rep(TRUE,nr) )
subpop <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,
                     Combined = rep("All Basins", nr), 
                     Basin = dsgn_sf$REPORTING_UNIT) 
dsgn <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, 
                   wgt = dsgn_sf$wgt,
                   xcoord = dsgn_sf$xcoord,
                   ycoord = dsgn_sf$ycoord,
                   stratum = dsgn_sf$REPORTING_UNIT)

data.cat <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,
                       TNTStatus=dsgn_sf$TNT,
                       EXCLUSION.CATEGORY = dsgn_sf$EXCLUSION_CATEGORY)

ExtentEst <- cat.analysis(sites, subpop, dsgn, data.cat, conf=95)

# write out or export results
write.csv(ExtentEst,file = 'ExtentEst.csv')

# Of the 384,373.86 LL hectares in the sample frame, 96.50% is estimated to be in the 
#  target population, i.e., 370,920.77 hectares. Also, only 77.82% of the sample frame 
#  could actually be sampled. To estimate the percent of the target population 
#  that could be sampled, requires that the analysis be stricted to just sites 
#  in the target population, i.e., TNT = "T"

sites <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, Use = dsgn_sf$TNT == "T" )
subpop <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,
                     Combined = rep("All Basins", nr), 
                     Basin = dsgn_sf$REPORTING_UNIT) 
dsgn <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, 
                   wgt = dsgn_sf$wgt,
                   xcoord = dsgn_sf$xcoord,
                   ycoord = dsgn_sf$ycoord,
                   stratum = dsgn_sf$REPORTING_UNIT)

data.cat <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,  
                       EXCLUSION.CATEGORY = dsgn_sf$EXCLUSION_CATEGORY)

ExtentEst_Target <- cat.analysis(sites, subpop, dsgn, data.cat, conf=95)

# write out or export results
write.csv(ExtentEst_Target, file = 'ExtentEst_Target.csv')

# 80.64% of the target population area could be sampled, i.e., 
#  299122.13 hectares CI(214,796.59 - 383,447.67 )hectares


#########################################################################################
#########################################################################################
# Water Quality Data

# Run function of FDEPgetdata package to pull result data. 
#
# Insert varible name between parentheses in function call below. The
#  function will pull the water resource for the water resource by year. 
#  For example large lake projects during year 2019 the enty would be "'LL19'"
#  Entering "'LL17','LL18','LL29'" for the variable will produce a dataframe for 
#  FDEP Status large lakes sampled 2018 - 2020.
#  Be sure to enclose in double and single quotes.
#
# Water resource acronyms are LL = large lakes, SL, = small lakes.

FDEPgetdata::getdata_results("'LL19'")

# Function getdata_results creates the table 'Results'.

# Create new data frame from the one just created.
LL_RSLTS<-Results
names(LL_RSLTS)

# Determine sample types in file.
addmargins(table(LL_RSLTS$SAMPLE_TYPE, LL_RSLTS$MATRIX, useNA = 'ifany'))

# Note that have BLANK, BOTTOM and PRIMARY data and matrix types. A total of 90 
#  sites sampled for water quality. Sites Z4-LL-13013 and Z5-LL-13014 had to be resampled for some reason. 
#  Only want to use PRIMARY results for population estimation.

# Water Quality Analyses
# Water quality analyses are based on 88 sites from the results file and must be merged with design information.

keep <- LL_RSLTS$SAMPLE_TYPE == 'PRIMARY' & LL_RSLTS$MATRIX == 'WATER'

# merge with exclusion file
LL_WQ <- merge(as.data.frame(dsgn_sf)[, c("PK_RANDOM_SAMPLE_LOCATION",
                                          "REPORTING_UNIT", "EXCLUSION_CATEGORY","TNT", "wgt", 
                                          "londd", "latdd", "xcoord", "ycoord", 
                                          "NUTRIENT_WATERSHED_REGION", "DO_Conc")], LL_RSLTS[keep,], 
               by.x = 'PK_RANDOM_SAMPLE_LOCATION', 
               by.y = 'FK_RANDOM_SAMPLE_LOCATION')

# check that have only PRIMARY for Water MATRIX data
addmargins(table(LL_WQ$SAMPLE_TYPE, LL_WQ$MATRIX, useNA = 'ifany'))


#########################################################################################

########Ammonia Calculation for threshold ####################

## Calculates total ammonia from single sample criteria 

## Code chunk written 1/6/2020 by Stephanie Sunderman Barnes.
## Calculator for total ammonia nitrogen (TAN) single sample criteria.
## Created using TAN calculator spreadsheet as a guide 
## (accessed 1/3/2020, https://floridadep.gov/dear/water-quality-standards-program/documents/total-ammonia-nitrogen-calculator%C2%A0).

##TotAmm.pH = pH used in ammonia calc. If measured pH < 6.5, value used is 6.5. If meas. pH > 9.0, value used is 9.0.
LL_WQ$TotAmm_pH <- ifelse(LL_WQ$pH_Field < 6.5, 6.5, 
                          ifelse(LL_WQ$pH_Field > 9.0, 9.0,LL_WQ$pH_Field))

##TotAmm.temp = water temperature used in ammonia calc. If measured temp < 7 degrees C, value used is 7.
LL_WQ$TotAmm_temp <- ifelse(LL_WQ$Water_Temperature < 7, 7, LL_WQ$Water_Temperature )

##calculate single sample Total Ammonia Criteria using TotAmm.pH and TotAmm.temp							
LL_WQ$TotAmmCrit_SingleSamp <- ifelse(is.na(LL_WQ$TotAmm_pH), NA,
                                      ifelse(is.na(LL_WQ$TotAmm_temp), NA,
                                             (2.5*(0.8876*((0.0278/(1+10^(7.688-LL_WQ$TotAmm_pH)))+(1.1994/(1+10^(LL_WQ$TotAmm_pH-7.688))))*2.126*10^(0.028*(20-(LL_WQ$TotAmm_temp)))))))

##Round result to two decimal places
LL_WQ$TotAmmCrit_SingleSamp <- round(LL_WQ$TotAmmCrit_SingleSamp, digits=2)

##Run analysis similar to NNC using the single sample total ammonia criteria calculated above
### Pass=1 AND Fail=0
LL_WQ$TAmm_Cat<-ifelse((LL_WQ$TotAmmCrit_SingleSamp >= LL_WQ$Ammonia_Total_as_N),1,0)

#########End Ammonia Calculation


###################################################################
### Numeric Nutrient and DO Categories
### Using the min and max NNC criteria values determine Pass=1 AND Fail=0 
### for the min and max NNC criteria values and for DO dissolved % saturation.

LL_WQ$Color_cat<- ifelse((LL_WQ$Color_true > 40) ,0,1)

LL_WQ$Alkalinity_cat<- ifelse((LL_WQ$Alkalinity_Total_as_CaCO3 > 20) ,0,1)

LL_WQ$Col_Alk_cat<-paste(LL_WQ$Color_cat, LL_WQ$Alkalinity_cat)

## Use new Col_Alk_cat character variable to assign thresholds for TN and TP
LL_WQ$TN_Max<- ifelse(LL_WQ$Col_Alk_cat=="0 0",2.23,
                      ifelse(LL_WQ$Col_Alk_cat=="0 1", 2.23,
                             ifelse(LL_WQ$Col_Alk_cat== "1 0", 1.91,
                                    ifelse(LL_WQ$Col_Alk_cat=="1 1", 0.93,NA))))
LL_WQ$TP_Max<- ifelse((LL_WQ$Color_cat==0 & LL_WQ$NUTRIENT_WATERSHED_REGION=="WEST CENTRAL"),0.49,
                      ifelse(LL_WQ$Col_Alk_cat=="0 0",0.16,
                             ifelse(LL_WQ$Col_Alk_cat=="0 1",0.16,
                                    ifelse(LL_WQ$Col_Alk_cat=="1 0", 0.09,
                                           ifelse(LL_WQ$Col_Alk_cat=="1 1",0.03,NA)))))

names(LL_WQ)

LL_WQ$TN<-(LL_WQ$Kjeldahl_Nitrogen_Total_as_N+LL_WQ$NitrateNitrite_Total_as_N)

### Pass=1 AND Fail=0
### Using the maximum values listed for Total nitrogen (F.A.C. 62-302.531)
###  and total phosphorus (F.A.C. 62-302.531), and the values for 
###  dissolved oxygen (F.A.C. 62-302.533)

LL_WQ$TN_cat<-ifelse((LL_WQ$TN_Max >= LL_WQ$TN),1,0) 

LL_WQ$TP_cat<-ifelse((LL_WQ$TP_Max >= LL_WQ$Phosphorus_Total_as_P),1,0)  	

LL_WQ$DO_cat<-ifelse((LL_WQ$DO_Conc >= LL_WQ$Oxygen_Dissolved_Percent_Saturation),0,1) 


### Note for combined NNc and DO, pass = 3, fail = 0
LL_WQ$NNCDO_cat<-(LL_WQ$TN_cat+LL_WQ$TP_cat+LL_WQ$DO_cat)


#LL_WQ$NNCDO_cat <- LL_WQ$NNCDO_tot
#LL_WQ$NNCDO_cat[LL_WQ$NNCDO_cat <= 2]<-0


########### end NNC and DO category calculations
################################################################

################################################################
### Chlorophyll Category calculations for lakes.

LL_WQ$Chlorophyll_conc<- ifelse(LL_WQ$Col_Alk_cat=="0 0",20,
                        ifelse(LL_WQ$Col_Alk_cat=="0 1", 20,
                               ifelse(LL_WQ$Col_Alk_cat== "1 0", 20,
                                      ifelse(LL_WQ$Col_Alk_cat=="1 1", 6,NA))))

LL_WQ$Chloride_cat<-ifelse((LL_WQ$Chlorophyll_conc > LL_WQ$Chlorophyll_A_Monochromatic),1,0) 

LL_WQ$Chloride_cat <- cut(LL_WQ$Chloride_cat, breaks=c(0,0.99,1), include.lowest=TRUE)
LL_WQ$Chloride_cat <- as.factor(LL_WQ$Chloride_cat)

### End chlorophyll calculations.


##########################################################################
#####  Set up threshold category columns for remainder of analytes

#### E_Coli category

E_Coli_cat <- cut(LL_WQ$Escherichia_Coli_Quanti_Tray, breaks=c(0,409,10000000), include.lowest=TRUE)
LL_WQ$E_Coli_cat <- E_Coli_cat
LL_WQ$E_Coli_cat <- as.factor(LL_WQ$E_Coli_cat)

### Old Dissolved Oxygen Category

DO_cat_old <- cut(LL_WQ$Oxygen_Dissolved_Field, breaks=c(0,4.999,1000000), include.lowest=TRUE)
LL_WQ$DO_cat_old <- DO_cat_old
LL_WQ$DO_cat_old <- as.factor(LL_WQ$DO_cat_old)

### pH Category

pH_cat <- cut(LL_WQ$pH_Field, breaks=c(0,5.999,8.5,14), include.lowest=TRUE)
LL_WQ$pH_cat <- pH_cat
LL_WQ$pH_cat <- as.factor(LL_WQ$pH_cat)

##### End setting up categories for water quality indicators

##########################################################################
# Continuous and categorical water quality indicator population estimation

nr <- nrow(LL_WQ)
levels(LL_WQ$TNT)

sites_WQ <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION, Use = LL_WQ$TNT == "T" )
subpop_WQ <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION,
                        Combined = rep("All Basins", nrow(LL_WQ)), 
                        Basin = LL_WQ$REPORTING_UNIT) 
dsgn_WQ <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION, 
                      wgt = LL_WQ$wgt,
                      xcoord = LL_WQ$xcoord,
                      ycoord = LL_WQ$ycoord,
                      stratum = LL_WQ$REPORTING_UNIT)

# continuous WQ estimates

data.cont.WQ <- data.frame(siteID=LL_WQ$PK_RANDOM_SAMPLE_LOCATION, 
                           LL_WQ[,c('Water_Temperature','pH_Field','Oxygen_Dissolved_Field',
                                    'Escherichia_Coli_Quanti_Tray', 'Chlorophyll_A_Monochromatic',
                                    'TAmm_Cat','TN','Phosphorus_Total_as_P')])
Water_quality_Cont <- cont.analysis(sites = sites_WQ, subpop = subpop_WQ, 
                                    design = dsgn_WQ, data.cont = data.cont.WQ, conf=95)

# categorical WQ estimates

data.cat.wq <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION,
                          Ammonia_Category = LL_WQ$TAmm_Cat,
                          TN_Category = LL_WQ$TN_cat,
                          TP_Category = LL_WQ$TP_cat,
                          DO_Category = LL_WQ$DO_cat,
                          TN_TP_DO_Category = LL_WQ$NNCDO_cat,
                          E_Coli_Category = LL_WQ$E_Coli_cat,
                          pH_Category = LL_WQ$pH_cat)

Water_Quality_Cat <- cat.analysis(sites = sites_WQ, subpop = subpop_WQ, 
                                  design = dsgn_WQ, data.cat = data.cat.wq, conf=95)


# write out the results 
write.csv(Water_Quality_Cat, "2019_LL_WQ_Cat.csv")
write.csv(Water_quality_Cont$CDF, file = '2019_LL_WQ_Cont_EstCDF.csv')
write.csv(Water_quality_Cont$Pct, file = '2019_LL_Cont_WQ_EstPCT.csv')

#### 
#######################################################################################
####  End of Water Quality Analyses
######################################################################################

#####################################################################
#####################################################################
###### Start sediment data analyses
####### Original code written July 27, 2020 by Jay Silvanima, 
#######   updated 8/20/2020 by Tony Olsen.


keep2 <- LL_RSLTS$SAMPLE_TYPE == 'PRIMARY' & LL_RSLTS$MATRIX == 'SEDIMENT'

# merge with exclusion file
LL_SED <- merge(as.data.frame(dsgn_sf)[, c("PK_RANDOM_SAMPLE_LOCATION",
                                          "REPORTING_UNIT", "EXCLUSION_CATEGORY","TNT", "wgt", 
                                          "londd", "latdd", "xcoord", "ycoord", "NUTRIENT_WATERSHED_REGION",
                                          "DO_Conc")], LL_RSLTS[keep2,], 
               by.x = 'PK_RANDOM_SAMPLE_LOCATION', 
               by.y = 'FK_RANDOM_SAMPLE_LOCATION')

# check that have only PRIMARY for sediment MATRIX data
addmargins(table(LL_SED$SAMPLE_TYPE, LL_SED$MATRIX, useNA = 'ifany'))

# Notice only 81 of the 88 sites sampled for water quality were able to be
#  sampled for sediments.


####### Create Predicted Effects categories for each indicator and for the total of all indicators
### 
LL_SED$AsPECcat <- ifelse(LL_SED$Arsenic_Sediments > 33, 1, 0)
LL_SED$AsPECcat[is.na(LL_SED$AsPECcat)] <- 0
LL_SED$CdPECcat <- ifelse(LL_SED$Cadmium_Sediments > 5, 1, 0)
LL_SED$CdPECcat[is.na(LL_SED$CdPECcat)] <- 0
LL_SED$CrPECcat <- ifelse(LL_SED$Chromium_Sediments > 111, 1, 0)
LL_SED$CrPECcat[is.na(LL_SED$CrPECcat)] <- 0
LL_SED$CuPECcat <- ifelse(LL_SED$Copper_Sediments > 149, 1, 0)
LL_SED$CuPECcat[is.na(LL_SED$CuPECcat)] <- 0
LL_SED$AgPECcat <- ifelse(LL_SED$Silver_Sediments > 2.2, 1, 0)
LL_SED$AgPECcat[is.na(LL_SED$AgPECcat)] <- 0
LL_SED$NiPECcat <- ifelse(LL_SED$Nickel_Sediments > 48, 1, 0)
LL_SED$NiPECcat[is.na(LL_SED$NiPECcat)] <- 0
LL_SED$PbPECcat <- ifelse(LL_SED$Lead_Sediments > 128, 1, 0)
LL_SED$PbPECcat[is.na(LL_SED$PbPECcat)] <- 0
LL_SED$HgPECcat <- ifelse(LL_SED$Mercury_Sediments > 1.06, 1, 0)
LL_SED$HgPECcat[is.na(LL_SED$HgPECcat)] <- 0
LL_SED$ZnPECcat <- ifelse(LL_SED$Zinc_Sediments > 459, 1, 0)
LL_SED$ZnPECcat[is.na(LL_SED$ZnPECcat)] <- 0

# Combined total PEC categories
LL_SED$NumExceedPECcat <- LL_SED$AsPECcat + LL_SED$CdPECcat + LL_SED$CrPECcat +
  LL_SED$CuPECcat + LL_SED$AgPECcat + LL_SED$NiPECcat +
  LL_SED$PbPECcat + LL_SED$HgPECcat + LL_SED$ZnPECcat 

# Sites that exceed at least one
LL_SED$Exceed1_PECcat <- LL_SED$NumExceedPECcat
LL_SED$Exceed1_PECcat[LL_SED$Exceed1_PECcat >= 1] <- 1

addmargins(table(NumExceedPECcat = LL_SED$NumExceedPECcat, 
                 Exceed1_PECcat = LL_SED$Exceed1_PECcat, useNA = 'ifany'))

# Sediment Category Population Estimation

sites_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, Use = LL_SED$TNT == "T" )
subpop_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                         Combined = rep("All Basins", nrow(LL_SED)), 
                         Basin = LL_SED$REPORTING_UNIT) 
dsgn_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, 
                       wgt = LL_SED$wgt,
                       xcoord = LL_SED$xcoord,
                       ycoord = LL_SED$ycoord,
                       stratum = LL_SED$REPORTING_UNIT)

data.cat.sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                           Num_Exceed_PEC_Category = LL_SED$NumExceedPECcat,
                           Exceed_1_PEC_Category = LL_SED$Exceed1_PECcat,
                           Arsenic_PEC_Category = LL_SED$AsPECcat, 
                           Cadmium_PEC_Category = LL_SED$CdPECcat,
                           Chromium_PEC_Category = LL_SED$CrPECcat,
                           Copper_PEC_Category = LL_SED$CuPECcat,
                           Iron_PEC_Category = LL_SED$AgPECcat,
                           Nickel_PEC_Category = LL_SED$NiPECcat,
                           Lead_PEC_Category = LL_SED$PbPECcat,
                           Mercury_PEC_Category = LL_SED$HgPECcat,
                           Zinc_PEC_Category = LL_SED$ZnPECcat)


Sediment_Cat <- cat.analysis(sites = sites_sed, subpop = subpop_sed, 
                             design = dsgn_sed, data.cat = data.cat.sed, conf=95)

# This analysis assumes that you only want to do estimates for sites sampled 
#  for metals. But 7 sites were sampled for water but not sediments and 
#  could be assigned a category value of "Not_Assessed". So categories 
#  would be 1, 0, Not_Assessed or could recode to be Exceeds, Meets and 
#  Not_Assessed.  Note if do this then would use all 88 sampled sites 
#  and not just the 81 sampled for sediments.

# Assignments to categories makes the assumption that if a value is missing, then it is below the criteria. This was not checked. It may be that some of these could be assigned as Not_assessed.

# Sediment continuous distribution estimation

sites_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, Use = LL_SED$TNT == "T" )
subpop_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                         Combined = rep("All Basins", nrow(LL_SED)), 
                         Basin = LL_SED$REPORTING_UNIT) 
dsgn_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, 
                       wgt = LL_SED$wgt,
                       xcoord = LL_SED$xcoord,
                       ycoord = LL_SED$ycoord,
                       stratum = LL_SED$REPORTING_UNIT)

# Create data frame with only sediment analytes to be used for CDFs
# Use names command to determine the column identifiers for the analytes
#  you'd like in the cont.analysis

names(LL_SED)

data.cont.sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                            LL_SED[, c(seq(152, 171, by=2),124,126,128,130,132,134,136,138)])

names(data.cont.sed)

Sediment_Cont <- cont.analysis(sites = sites_sed, subpop = subpop_sed, 
                               design = dsgn_sed, data.cont = data.cont.sed, conf=95)


# write out the results 
write.csv(Sediment_Cat, "2019_LL_SED_Cat.csv")
write.csv(Sediment_Cont$CDF, file = '2019_LL_SED_Cont_EstCDF.csv')
write.csv(Sediment_Cont$Pct, file = '2019_LL_SED_Cont_EstPCT.csv')

###########################################################################
#### End of large lakes analyses.
###########################################################################