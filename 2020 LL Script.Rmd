---
title: "2020 Large Lakes (LL) Script"
author: "Jay Silvanima"
date: "`r format(Sys.time(),  '%B %d, %Y')`"
output:
  html_document: default
  pdf_document: default
---
This RMarkdown script is based on the R script "2020 LL script.R" 

Note that this code can be executed in current environment by using the code within the ```{r} to ``` portions.  It can also be executed by using "knit" in RStudio which will produce an html file of the output.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Load libraries and set working directory or path.
 
```{r library and path}
# library(FDEPgetdata)
library(spsurvey)
library(sf)
library(ggplot2)
setwd("C:/R/Status 2020/2020 LL")
```
## Purpose:
Analysis of Florida Large Lake data generated by the Status 
 Monitoring Program. Created by Jay Silvanima using code developed 
 by Tony Olsen, myself, Chris Sedlacek, Stephanie Sunderman-Barnes, 
 and Liz Miller on 09-18-2020.  
  
Code developed using R version 3.6.2 (2019-12-12). Note that the script uses  version 4.1.0 of spsurvey which requires use of "sf" and "sp" packages.

## Load Exclusions File
Insert the name of the exclusion table created in FDEP's Oracle database
 'GWIS'. As this is a literal, be sure to enclose in single quotes. Example 'CN_EXCLUSIONS_2020'. # Function getdata_results creates the table 'Exclusions'.
 
For this markdown file we'll read in the export of the data pull above because the package FDEPgetdata is an internal FDEP package.
 
```{r} 

#FDEPgetdata::getdata_fw_exclusions('CN_EXCLUSIONS_2019')

# Package FDEPgetdata creates a dataframe names 'Exclusions' from the 
#  information provided.

LL.SITES <- read.csv("C:/R/Status 2020/2020 LL/LL_EXCLUSIONS_2019.CSV",
                     stringsAsFactors = FALSE)
names(LL.SITES)
head(LL.SITES)
 
```
Only location information that is available for all sites is RANDOM_LATITUDE and RANDOM_LONGITUDE which must be converted to decimal degrees.

```{r latlon}
# Convert to Decimal degrees and do map projection

deg <- floor(LL.SITES$RANDOM_LATITUDE/10000)
min <- floor((LL.SITES$RANDOM_LATITUDE - deg*10000)/100)
sec <- LL.SITES$RANDOM_LATITUDE - deg*10000 - min*100
LL.SITES$latdd <- deg + min/60 + sec/3600
deg <- floor(LL.SITES$RANDOM_LONGITUDE/10000)
min <- floor((LL.SITES$RANDOM_LONGITUDE - deg*10000)/100)
sec <- LL.SITES$RANDOM_LONGITUDE - deg*10000 - min*100
LL.SITES$londd <- deg + min/60 + sec/3600

# Change londd to negative for correct use in sf.
LL.SITES$londd <- -LL.SITES$londd
```
Per Tony Olsen, "spsurvey in the future will use sf objects and will not have functions to convert to projected coordinates except by use of sf library functions. Consequently, create an sf design object, transform to an Albers projection and keep both lat/lon and xycoords as variables in the data.frame. Note that crs equal to EPSG code 4152 is NAD83 HARN datum for lat/lon and EPSG code 5070 is an Albers projection. Can do web search for these to see their definition.

Note the plot command is a "method" for sf package to plot information. In this case, all we plot is the site locations, i.e., their geometry."

```{r}
# Create sf object and transform to Albers projection for analysis
#  This codes utilizes Coordinate Reference System (CRS) Codes.
#  The first crs code below is for NAD 83 Harn datum the second crs code
#  is for Florida albers projection. More information on these codes is found here:
#  https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf.

dsgn_LL <- st_as_sf(LL.SITES, coords = c("londd", "latdd"), remove = FALSE,
                    crs = 4152)
dsgn_sf <- st_transform(dsgn_LL, crs = 3087)

# keep xy coords as variables
tmp <- st_coordinates(dsgn_sf)
dsgn_sf$xcoord <- tmp[, "X"]
dsgn_sf$ycoord <- tmp[, "Y"]

# plot sites using sf
plot(st_geometry(dsgn_sf))
```

# Site Evaluation
The variables CAN_BE_SAMPLED, EXCLUSION_CATEGORY and EXCLUSION_CRITERIA provide information on the site evaluation results for each site. Review the information and create target/nontarget (TNT) variable.

```{r}
addmargins(table(dsgn_sf$EXCLUSION_CATEGORY, dsgn_sf$CAN_BE_SAMPLED, useNA = 'ifany'))
addmargins(table(dsgn_sf$EXCLUSION_CRITERIA, useNA = 'ifany'))


# create sampled and target (T) / nontarget (NT) variables

dsgn_sf$EXCLUSION_CATEGORY <- as.character(dsgn_sf$EXCLUSION_CATEGORY)
dsgn_sf$EXCLUSION_CATEGORY[dsgn_sf$CAN_BE_SAMPLED == 'Y'] <- 'SAMPLED'
dsgn_sf$EXCLUSION_CATEGORY <- as.factor(dsgn_sf$EXCLUSION_CATEGORY)
levels(dsgn_sf$EXCLUSION_CATEGORY)
dsgn_sf$TNT <- dsgn_sf$EXCLUSION_CATEGORY
levels(dsgn_sf$TNT) <- list(T=c('SAMPLED', 'NO PERMISSION FROM OWNER', 'UNABLE TO ACCESS','OTHERWISE UNSAMPLEABLE','DRY'),
                            NT=c('WRONG RESOURCE/NOT PART OF TARGET POPULATION') )

addmargins(table(dsgn_sf$EXCLUSION_CATEGORY, dsgn_sf$TNT, useNA = 'ifany'))
```
A total of 116 large lake sites were evaluated with 88 sites actually sampled. Eight sites where determined to not meet the definition of our target poplutation of lakes.

# Adjust Weights for Design As Implemented
Per Tony Olsen, "Since the survey design implementation required the use of replacement sites, the initial design weights must be adjusted to account for the actual number of sites evaluated. The weight adjustment requires knowing the lake area in the sample frame. First, create the framesize using lake hectares per zone from design document. Note that if prefer to have the lake area in units other than hectares, then best option is to do the conversion on framesize. For example, multiple framesize by 2.471 to convert to acres. The initial design weights are in "NEST1_WT" and the weight adjustment categories are the six reporting units (or basins)."

Note need frame size here found in design doc for latge lake site selections. From design document framesize in hectates = 384,373.86 for entire data frame.

```{r}
framesize <- c("ZONE 1"=18319.077 ,"ZONE 2"=7646.538 ,"ZONE 3"=121141.645,
               "ZONE 4"=43936.962 ,"ZONE 5"=63328.456 ,"ZONE 6"=130001.179)

# use all evaluated sites to adjust weights
nr <- nrow(dsgn_sf)
dsgn_sf$wgt <- adjwgt(rep(TRUE,nr), dsgn_sf$NEST1_WT, 
                      dsgn_sf$REPORTING_UNIT, framesize=framesize)

# check sum of weights for each reporting unit/basin
addmargins(tapply(dsgn_sf$wgt, dsgn_sf$REPORTING_UNIT, sum))

```
This gives the weights for the large lake design as implememented in 2019. It must include all evaluated sites as some sites are not in the target population.

# Estimate Extent Lake Area
Since the sample frame includes portions of LL object polygons that do not meet the definition of a LL, the site evaluation information is used 
to estimate the LL hectares in the target population for entire state and for each of the reporting units/basins.

```{r}
sites <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, Use=rep(TRUE,nr) )
subpop <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,
                     Combined = rep("All Basins", nr), 
                     Basin = dsgn_sf$REPORTING_UNIT) 
dsgn <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, 
                   wgt = dsgn_sf$wgt,
                   xcoord = dsgn_sf$xcoord,
                   ycoord = dsgn_sf$ycoord,
                   stratum = dsgn_sf$REPORTING_UNIT)

data.cat <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,
                       TNTStatus=dsgn_sf$TNT,
                       EXCLUSION.CATEGORY = dsgn_sf$EXCLUSION_CATEGORY)

ExtentEst <- cat.analysis(sites, subpop, dsgn, data.cat, conf=95)

# write out or export results
write.csv(ExtentEst,file = 'ExtentEst.csv')
```
Of the 384,373.86 LL hectares in the sample frame, 96.50% is estimated to be in the target population, i.e., 370,920.77 hectares. Also, only 77.82% of the sample frame could actually be sampled. To estimate the percent of the target population that could be sampled, requires that the analysis be stricted to just sites in the target population, i.e., TNT = "T"

```{r}

sites <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, Use = dsgn_sf$TNT == "T" )
subpop <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,
                     Combined = rep("All Basins", nr), 
                     Basin = dsgn_sf$REPORTING_UNIT) 
dsgn <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION, 
                   wgt = dsgn_sf$wgt,
                   xcoord = dsgn_sf$xcoord,
                   ycoord = dsgn_sf$ycoord,
                   stratum = dsgn_sf$REPORTING_UNIT)

data.cat <- data.frame(siteID = dsgn_sf$PK_RANDOM_SAMPLE_LOCATION,  
                       EXCLUSION.CATEGORY = dsgn_sf$EXCLUSION_CATEGORY)

ExtentEst_Target <- cat.analysis(sites, subpop, dsgn, data.cat, conf=95)

# write out or export results
write.csv(ExtentEst_Target, file = 'ExtentEst_Target.csv')
```
80.64% of the target population area could be sampled, i.e., 
299,122.13 hectares CI(214,796.59 - 383,447.67 )hectares

# Water Quality and Sediment Data Pull
Run function of FDEPgetdata package to pull result data. Insert varible name between parentheses in function call below. The function will pull the water resource for the water resource by year. For example large lake projects during year 2019 the enty would be "'LL19'" Entering "'LL17','LL18','LL29'" for the variable will produce a dataframe for FDEP Status large lakes sampled 2018 - 2020. Be sure to enclose in double and single quotes. Water resource acronyms are LL = large lakes, SL, = small lakes. 

For this markdown file we'll read in the export of the data pull above because the package FDEPgetdata is an internal FDEP package.

```{r}
# FDEPgetdata::getdata_results("'LL19'")

# Function getdata_results creates the dataframe 'Results'.

# Create new data frame from the one just created.
LL_RSLTS <- read.csv("C:/R/Status 2020/2020 LL/LL_RESULTS_2019.CSV", 
                    stringsAsFactors = FALSE)
names(LL_RSLTS)

# Determine sample types in file.
addmargins(table(LL_RSLTS$SAMPLE_TYPE, LL_RSLTS$MATRIX, useNA = 'ifany'))
```
Note that have BLANK, BOTTOM and PRIMARY data and matrix types. A total of 90 sites sampled for water quality. Sites Z4-LL-13013 and Z5-LL-13014 had to be resampled for some reason. Only want to use PRIMARY results for population estimation.

# Water Quality Analyses
Water quality analyses are based on 88 sites from the results file and must be merged with design information.

```{r}
keep <- LL_RSLTS$SAMPLE_TYPE == 'PRIMARY' & LL_RSLTS$MATRIX == 'WATER'

# merge with exclusion file
LL_WQ <- merge(as.data.frame(dsgn_sf)[, c("PK_RANDOM_SAMPLE_LOCATION",
                                          "REPORTING_UNIT", "EXCLUSION_CATEGORY","TNT", "wgt", 
                                          "londd", "latdd", "xcoord", "ycoord", "NUTRIENT_WATERSHED_REGION",
                                          "DO_Conc")], LL_RSLTS[keep,], 
               by.x = 'PK_RANDOM_SAMPLE_LOCATION', 
               by.y = 'FK_RANDOM_SAMPLE_LOCATION')

# check that have only PRIMARY for Water MATRIX data
addmargins(table(LL_WQ$SAMPLE_TYPE, LL_WQ$MATRIX, useNA = 'ifany'))
```
## Ammonia Calculation for Threshold
Calculates total ammonia from single sample criteria 

Code chunk written 1/6/2020 by Stephanie Sunderman Barnes. Calculator for total ammonia nitrogen (TAN) single sample criteria. Created using TAN calculator spreadsheet as a guide (accessed 1/3/2020, https://floridadep.gov/dear/water-quality-standards-program/documents/total-ammonia-nitrogen-calculator%C2%A0).

```{r}
##TotAmm.pH = pH used in ammonia calc. If measured pH < 6.5, value used is 6.5. If meas. pH > 9.0, value used is 9.0.
LL_WQ$TotAmm_pH <- ifelse(LL_WQ$pH_Field < 6.5, 6.5, 
                          ifelse(LL_WQ$pH_Field > 9.0, 9.0,LL_WQ$pH_Field))

##TotAmm.temp = water temperature used in ammonia calc. If measured temp < 7 degrees C, value used is 7.
LL_WQ$TotAmm_temp <- ifelse(LL_WQ$Water_Temperature < 7, 7, LL_WQ$Water_Temperature )

##calculate single sample Total Ammonia Criteria using TotAmm.pH and TotAmm.temp							
LL_WQ$TotAmmCrit_SingleSamp <- ifelse(is.na(LL_WQ$TotAmm_pH), NA,
                                      ifelse(is.na(LL_WQ$TotAmm_temp), NA,
                                             (2.5*(0.8876*((0.0278/(1+10^(7.688-LL_WQ$TotAmm_pH)))+(1.1994/(1+10^(LL_WQ$TotAmm_pH-7.688))))*2.126*10^(0.028*(20-(LL_WQ$TotAmm_temp)))))))

##Round result to two decimal places
LL_WQ$TotAmmCrit_SingleSamp <- round(LL_WQ$TotAmmCrit_SingleSamp, digits=2)

##Run analysis similar to NNC using the single sample total ammonia criteria calculated above
### Pass=1 AND Fail=0
LL_WQ$TAmm_Cat<-ifelse((LL_WQ$TotAmmCrit_SingleSamp >= LL_WQ$Ammonia_Total_as_N),1,0)

#########End Ammonia Calculation
```
## Florida Numeric Nutrient and DO Criteria Categories
Using the min and max NNC criteria values determine Pass=1 AND Fail=0 for the min and max NNC criteria values and for DO dissolved % saturation.

```{r}
LL_WQ$Color_cat<- ifelse((LL_WQ$Color_true > 40) ,0,1)

LL_WQ$Alkalinity_cat<- ifelse((LL_WQ$Alkalinity_Total_as_CaCO3 > 20) ,0,1)

LL_WQ$Col_Alk_cat<-paste(LL_WQ$Color_cat, LL_WQ$Alkalinity_cat)

## Use new Col_Alk_cat character variable to assign thresholds for TN and TP
LL_WQ$TN_Min<- ifelse(LL_WQ$Col_Alk_cat=="0 0",1.27,
                      ifelse(LL_WQ$Col_Alk_cat=="0 1", 1.27,
                             ifelse(LL_WQ$Col_Alk_cat== "1 0", 1.05,
                                    ifelse(LL_WQ$Col_Alk_cat=="1 1", 0.51,NA))))
LL_WQ$TN_Max<- ifelse(LL_WQ$Col_Alk_cat=="0 0",2.23,
                      ifelse(LL_WQ$Col_Alk_cat=="0 1", 2.23,
                             ifelse(LL_WQ$Col_Alk_cat== "1 0", 1.91,
                                    ifelse(LL_WQ$Col_Alk_cat=="1 1", 0.93,NA))))
LL_WQ$TP_Min<- ifelse((LL_WQ$Color_cat==0 & LL_WQ$NUTRIENT_WATERSHED_REGION=="WEST CENTRAL"),0.49,
                      ifelse(LL_WQ$Col_Alk_cat=="0 0",0.05,
                             ifelse(LL_WQ$Col_Alk_cat=="0 1",0.05,
                                    ifelse(LL_WQ$Col_Alk_cat=="1 0", 0.03,
                                           ifelse(LL_WQ$Col_Alk_cat=="1 1",0.01,NA)))))
LL_WQ$TP_Max<- ifelse((LL_WQ$Color_cat==0 & LL_WQ$NUTRIENT_WATERSHED_REGION=="WEST CENTRAL"),0.49,
                      ifelse(LL_WQ$Col_Alk_cat=="0 0",0.16,
                             ifelse(LL_WQ$Col_Alk_cat=="0 1",0.16,
                                    ifelse(LL_WQ$Col_Alk_cat=="1 0", 0.09,
                                           ifelse(LL_WQ$Col_Alk_cat=="1 1",0.03,NA)))))


LL_WQ$TN<-(LL_WQ$Kjeldahl_Nitrogen_Total_as_N+LL_WQ$NitrateNitrite_Total_as_N)

### Pass=1 AND Fail=0
### Using the minimum values

LL_WQ$TN_cat<-ifelse((LL_WQ$TN_Min >= LL_WQ$TN),1,0) 

LL_WQ$TP_cat<-ifelse((LL_WQ$TP_Min >= LL_WQ$Phosphorus_Total_as_P),1,0)  	

LL_WQ$DO_cat<-ifelse((LL_WQ$DO_Conc >= 
        LL_WQ$Oxygen_Dissolved_Percent_Saturation),0,1) 


### Note for combined NNc and DO, pass = 3, fail = 0
LL_WQ$NNCDO_cat<-(LL_WQ$TN_cat+LL_WQ$TP_cat+LL_WQ$DO_cat)


#LL_WQ$NNCDO_cat <- LL_WQ$NNCDO_tot
#LL_WQ$NNCDO_cat[LL_WQ$NNCDO_cat <= 2]<-0


########### end NNC and DO category calculations
```

## Florida Chlorophyll Category Calculations for Lakes.

```{r}
LL_WQ$Chlorophyll_conc<- ifelse(LL_WQ$Col_Alk_cat=="0 0",20,
                        ifelse(LL_WQ$Col_Alk_cat=="0 1", 20,
                               ifelse(LL_WQ$Col_Alk_cat== "1 0", 20,
                                      ifelse(LL_WQ$Col_Alk_cat=="1 1", 6,NA))))

LL_WQ$Chloride_cat<-ifelse((LL_WQ$Chlorophyll_conc > LL_WQ$Chlorophyll_A_Monochromatic),1,0) 

LL_WQ$Chloride_cat <- cut(LL_WQ$Chloride_cat, breaks=c(0,0.99,1), include.lowest=TRUE)
LL_WQ$Chloride_cat <- as.factor(LL_WQ$Chloride_cat)

### End chlorophyll calculations.
```
## Set up Threshold Categies for Remainder of Analytes

### E_Coli category

```{r}
E_Coli_cat <- cut(LL_WQ$Escherichia_Coli_Quanti_Tray, breaks=c(0,409,10000000), include.lowest=TRUE)
LL_WQ$E_Coli_cat <- E_Coli_cat
LL_WQ$E_Coli_cat <- as.factor(LL_WQ$E_Coli_cat)
```
### Old Dissolved Oxygen Category

```{r}
DO_cat_old <- cut(LL_WQ$Oxygen_Dissolved_Field, breaks=c(0,4.999,1000000), include.lowest=TRUE)
LL_WQ$DO_cat_old <- DO_cat_old
LL_WQ$DO_cat_old <- as.factor(LL_WQ$DO_cat_old)
```
### pH Category

```{r}
pH_cat <- cut(LL_WQ$pH_Field, breaks=c(0,5.999,8.5,14), include.lowest=TRUE)
LL_WQ$pH_cat <- pH_cat
LL_WQ$pH_cat <- as.factor(LL_WQ$pH_cat)
```
End setting up categories for water quality indicators

# Water Quality Continuous Population Estimation 

```{r}
nr <- nrow(LL_WQ)
levels(LL_WQ$TNT)

sites_WQ <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION, Use = LL_WQ$TNT == "T" )
subpop_WQ <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION,
                        Combined = rep("All Basins", nrow(LL_WQ)), 
                        Basin = LL_WQ$REPORTING_UNIT) 
dsgn_WQ <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION, 
                      wgt = LL_WQ$wgt,
                      xcoord = LL_WQ$xcoord,
                      ycoord = LL_WQ$ycoord,
                      stratum = LL_WQ$REPORTING_UNIT)

names(LL_WQ)

data.cont.WQ <- data.frame(siteID=LL_WQ$PK_RANDOM_SAMPLE_LOCATION,        LL_WQ[,c('Water_Temperature','pH_Field','Oxygen_Dissolved_Field','Escherichia_Coli_Quanti_Tray','Chlorophyll_A_Monochromatic','TAmm_Cat','TN','Phosphorus_Total_as_P')])

Water_quality_Cont <- cont.analysis(sites = sites_WQ, subpop = subpop_WQ, 
                                    design = dsgn_WQ, data.cont = data.cont.WQ, conf=95)
```
# Water Quality Categorical Distribution Estimation

```{r}
data.cat.wq <- data.frame(siteID = LL_WQ$PK_RANDOM_SAMPLE_LOCATION,
                          Ammonia_Category = LL_WQ$TAmm_Cat,
                          TN_Category = LL_WQ$TN_cat,
                          TP_Category = LL_WQ$TP_cat,
                          DO_Category = LL_WQ$DO_cat,
                          TN_TP_DO_Category = LL_WQ$NNCDO_cat,
                          E_Coli_Category = LL_WQ$E_Coli_cat,
                          pH_Category = LL_WQ$pH_cat)

Water_Quality_Cat <- cat.analysis(sites = sites_WQ, subpop = subpop_WQ, 
                                  design = dsgn_WQ, data.cat = data.cat.wq, conf=95)
```
# Write Out Water Quality Results

```{r}
write.csv(Water_Quality_Cat, "2019_LL_WQ_Cat.csv")
write.csv(Water_quality_Cont$CDF, file = '2019_LL_WQ_Cont_EstCDF.csv')
write.csv(Water_quality_Cont$Pct, file = '2019_LL_Cont_WQ_EstPCT.csv')
```
End of Water Quality Analyses

# Sediment Quality Analyses
Original code written July 27, 2020 by Jay Silvanima, 
updated 8/20/2020 by Tony Olsen and then again by Jay Silvanima 10/02/2020.

```{r}
keep2 <- LL_RSLTS$SAMPLE_TYPE == 'PRIMARY' & LL_RSLTS$MATRIX == 'SEDIMENT'

# merge with exclusion file
LL_SED <- merge(as.data.frame(dsgn_sf)[, c("PK_RANDOM_SAMPLE_LOCATION",
                                          "REPORTING_UNIT", "EXCLUSION_CATEGORY","TNT", "wgt", 
                                          "londd", "latdd", "xcoord", "ycoord", "NUTRIENT_WATERSHED_REGION",
                                          "DO_Conc")], LL_RSLTS[keep2,], 
               by.x = 'PK_RANDOM_SAMPLE_LOCATION', 
               by.y = 'FK_RANDOM_SAMPLE_LOCATION')

# check that have only PRIMARY for sediment MATRIX data
addmargins(table(LL_SED$SAMPLE_TYPE, LL_SED$MATRIX, useNA = 'ifany'))
```

Notice only 81 of the 88 sites sampled for water quality were able to be
sampled for sediments.


# Create Sediment Predicted Effects Categories 

```{r}
LL_SED$AsPECcat <- ifelse(LL_SED$Arsenic_Sediments > 33, 1, 0)
LL_SED$AsPECcat[is.na(LL_SED$AsPECcat)] <- 0
LL_SED$CdPECcat <- ifelse(LL_SED$Cadmium_Sediments > 5, 1, 0)
LL_SED$CdPECcat[is.na(LL_SED$CdPECcat)] <- 0
LL_SED$CrPECcat <- ifelse(LL_SED$Chromium_Sediments > 111, 1, 0)
LL_SED$CrPECcat[is.na(LL_SED$CrPECcat)] <- 0
LL_SED$CuPECcat <- ifelse(LL_SED$Copper_Sediments > 149, 1, 0)
LL_SED$CuPECcat[is.na(LL_SED$CuPECcat)] <- 0
LL_SED$AgPECcat <- ifelse(LL_SED$Silver_Sediments > 2.2, 1, 0)
LL_SED$AgPECcat[is.na(LL_SED$AgPECcat)] <- 0
LL_SED$NiPECcat <- ifelse(LL_SED$Nickel_Sediments > 48, 1, 0)
LL_SED$NiPECcat[is.na(LL_SED$NiPECcat)] <- 0
LL_SED$PbPECcat <- ifelse(LL_SED$Lead_Sediments > 128, 1, 0)
LL_SED$PbPECcat[is.na(LL_SED$PbPECcat)] <- 0
LL_SED$HgPECcat <- ifelse(LL_SED$Mercury_Sediments > 1.06, 1, 0)
LL_SED$HgPECcat[is.na(LL_SED$HgPECcat)] <- 0
LL_SED$ZnPECcat <- ifelse(LL_SED$Zinc_Sediments > 459, 1, 0)
LL_SED$ZnPECcat[is.na(LL_SED$ZnPECcat)] <- 0

# Combined total PEC categories
LL_SED$NumExceedPECcat <- LL_SED$AsPECcat + LL_SED$CdPECcat + LL_SED$CrPECcat +
  LL_SED$CuPECcat + LL_SED$AgPECcat + LL_SED$NiPECcat +
  LL_SED$PbPECcat + LL_SED$HgPECcat + LL_SED$ZnPECcat 

# Sites that exceed at least one
LL_SED$Exceed1_PECcat <- LL_SED$NumExceedPECcat
LL_SED$Exceed1_PECcat[LL_SED$Exceed1_PECcat >= 1] <- 1

addmargins(table(NumExceedPECcat = LL_SED$NumExceedPECcat, 
                 Exceed1_PECcat = LL_SED$Exceed1_PECcat, useNA = 'ifany'))
```
# Sediment Category Population Estimation

```{r}
sites_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, Use = LL_SED$TNT == "T" )
subpop_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                         Combined = rep("All Basins", nrow(LL_SED)), 
                         Basin = LL_SED$REPORTING_UNIT) 
dsgn_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, 
                       wgt = LL_SED$wgt,
                       xcoord = LL_SED$xcoord,
                       ycoord = LL_SED$ycoord,
                       stratum = LL_SED$REPORTING_UNIT)

data.cat.sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                           Num_Exceed_PEC_Category = LL_SED$NumExceedPECcat,
                           Exceed_1_PEC_Category = LL_SED$Exceed1_PECcat,
                           Arsenic_PEC_Category = LL_SED$AsPECcat, 
                           Cadmium_PEC_Category = LL_SED$CdPECcat,
                           Chromium_PEC_Category = LL_SED$CrPECcat,
                           Copper_PEC_Category = LL_SED$CuPECcat,
                           Iron_PEC_Category = LL_SED$AgPECcat,
                           Nickel_PEC_Category = LL_SED$NiPECcat,
                           Lead_PEC_Category = LL_SED$PbPECcat,
                           Mercury_PEC_Category = LL_SED$HgPECcat,
                           Zinc_PEC_Category = LL_SED$ZnPECcat)


Sediment_Cat <- cat.analysis(sites = sites_sed, subpop = subpop_sed, 
                             design = dsgn_sed, data.cat = data.cat.sed, conf=95)
```
This analysis assumes that you only want to do estimates for sites sampled for metals. But 7 sites were sampled for water but not sediments and could be assigned a category value of "Not_Assessed". So categories would be 1, 0, Not_Assessed or could recode to be Exceeds, Meets and Not_Assessed. Note if do this then would use all 88 sampled sites and not just the 81 sampled for sediments.

Assignments to categories makes the assumption that if a value is missing, then it is below the criteria. This was not checked. It may be that some of these could be assigned as Not_assessed.

# Sediment continuous distribution estimation

```{r}
sites_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, Use = LL_SED$TNT == "T" )

subpop_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                         Combined = rep("All Basins", nrow(LL_SED)), 
                         Basin = LL_SED$REPORTING_UNIT) 
dsgn_sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION, 
                       wgt = LL_SED$wgt,
                       xcoord = LL_SED$xcoord,
                       ycoord = LL_SED$ycoord,
                       stratum = LL_SED$REPORTING_UNIT)

# Create data frame with only sediment analytes to be used for CDFs
# Use names command to determine the column identifiers for the analytes
#  you'd like in the cont.analysis

names(LL_SED)

data.cont.sed <- data.frame(siteID = LL_SED$PK_RANDOM_SAMPLE_LOCATION,
                            LL_SED[, c(seq(119, 140, by=2),155,157,161,163,165,167,169,171)])

names(data.cont.sed)

Sediment_Cont <- cont.analysis(sites = sites_sed, subpop = subpop_sed, 
                               design = dsgn_sed, data.cont = data.cont.sed, conf=95)
```
# Write Out Sediment Results

```{r}
write.csv(Sediment_Cat, "2019_LL_SED_Cat.csv")
write.csv(Sediment_Cont$CDF, file = '2019_LL_SED_Cont_EstCDF.csv')
write.csv(Sediment_Cont$Pct, file = '2019_LL_SED_Cont_EstPCT.csv')
```

# End of large lakes analyses.